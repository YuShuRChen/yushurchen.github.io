---
---

@unpublished{llmath,
  author="Chen, Yu-Shu",
  title="A Study of LLMs' Ability to Perform Basic Math Operations in Different Bases",
  year="2024",

  abstract={Large Language Models have demonstrated remarkable proficiency in a variety of natural language processing tasks, including translation, question-answering, and fill-in-the-blank exercises, with some evidence of reasoning abilities. In this paper, I explore their mathematical reasoning abilities through their performance on simple mathematical tasks. By evaluating GPT-3.5 turbo and GPT-4o on tasks involving addition, subtraction, and multiplication in uncommon numerical bases, this study aims to shed light on the models' ability to generalize to unfamiliar data and their underlying reasoning potential. The results offer insights into the strengths and limitations of LLMs in performing arithmetic operations outside the familiar base-10 system, contributing to a deeper understanding of their reasoning capacities. },
  preview={llmath.jpg},
  code={https://github.com/YuShuRChen/llmath.git},
  pdf={A_Study_of_Large_Language_Models__Ability_to_Perform_Basic_Math_Operations_in_Different_Bases.pdf},
  selected={true},
  website={https://arxiv.org/submit/5990735/view},
}

@unpublished{isnn,
  author="Chen, Yu-Shu and Meza, Andy",
  title="Interpretable Spiking Neural Network for Basic Mathematical Operations",
  year="2024",

  abstract={This project focuses on the integration of memory as a crucial component in logical reasoning tasks, particularly in arithmetic. I am investigating several neural architectures that emulate human memory systems, with the current emphasis on developing a spiking neural network (SNN) designed to function as a declarative memory network. This network serves as a long-term memory system capable of storing and recalling single-digit arithmetic operations. The overarching goal of this work is to integrate the declarative memory network with a procedural memory network, forming a larger, cohesive system. This integrated system aims to perform arithmetic tasks logically by leveraging declarative knowledge for fact retrieval and procedural reasoning for computation. Through this project, I aim to better understand the role of memory in reasoning and explore how biologically inspired networks, like SNNs, can be applied to enhance artificial intelligence systems' capacity for logical reasoning and task generalization. },
  selected={true},
  website={https://arxiv.org/submit/5990735/view},
}

@misc{cv4pid,
  author="Meza, Andy and Chen, Yu-Shu and Kaufhold, Stephan",
  title="Computer Vision for Primate Identification",
  year="2025",

  abstract={This project focuses on developing a computer vision system for identifying individual primates, leveraging a combination of synthetic datasets and real-world video footage. The synthetic dataset, generated to simulate diverse environmental conditions and primate poses, is used to augment the limited availability of labeled real-world footage. By integrating these two data sources, the model learns to generalize across varying scenarios, improving its ability to accurately identify primates in naturalistic settings. Key components of the project include dataset creation and augmentation, designing and training deep learning models, and evaluating performance in terms of accuracy and robustness. This work has implications for wildlife research, conservation efforts, and advancing techniques for dataset synthesis in machine learning. },
  selected={true},
}

@misc{holmes,
  author="Meza, Andy and Chen, Yu-Shu and Nwughala, Chima Tochukwu",
  title="HoLmeS: Design Space Exploration for High-Level Synthesis",
  year="2025",

  abstract={Given a set of optimization constraints, design space exploration (DSE) aims to efficiently find optimal solutions within a design space that is too large to exhaustively explore in a practical amount of time. When creating hardware designs using high-level synthesis (HLS), HLS designers have a multitude of "knobs" (i.e., compiler directives) that they can use to tune the properties of the compiler-produced design. Since these properties are often at odds with one another (e.g., performance vs. area) and a single HLS design can take hours to synthesize, the process of finding the optimal set of directive configurations requires a powerful DSE framework. Sherlock is a flexible DSE framework for finding Pareto-optimal solutions in a variety of design spaces. In this work, we evaluate Sherlock’s performance against that of other state-of-the-art DSE frameworks on DB4HLS, a comprehensive/standardized database of DSEs comprising over 100,000 HLS design points. Alongside this empirical evaluation, we introduce a number of extensions to Sherlock’s DSE algorithms to improve its performance within the HLS domain. We package these improvements as a new HLS-specific DSE framework named HoLmeS. },
  selected={true},
}
